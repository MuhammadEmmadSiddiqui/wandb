{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dcf9dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c306070f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\xt\\_netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"")\n",
    "  # Replace with your actual API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24a9ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "  }  # Random search    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c2ecee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'name': 'loss',\n",
    "    'goal': 'minimize'\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e5988d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'values': ['adam', 'sgd']\n",
    "    },\n",
    "    'fc_layer_size': {\n",
    "        'values': [128, 256, 512]\n",
    "    },\n",
    "    'dropout': {\n",
    "        'values': [0.2, 0.3, 0.4]\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88c7bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_dict.update({\n",
    "    'epochs': {\n",
    "        'value': 5\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2576d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_dict.update({\n",
    "    'learning_rate': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0.0001,\n",
    "        'max': 0.1\n",
    "    },\n",
    "    'batch_size': {\n",
    "        'distribution': 'q_log_uniform',\n",
    "        'min': 4,\n",
    "        'max': 128,\n",
    "        'q': 4\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30a0d7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'random',\n",
      " 'parameters': {'batch_size': {'distribution': 'q_log_uniform',\n",
      "                               'max': 128,\n",
      "                               'min': 4,\n",
      "                               'q': 4},\n",
      "                'dropout': {'values': [0.2, 0.3, 0.4]},\n",
      "                'epochs': {'value': 5},\n",
      "                'fc_layer_size': {'values': [128, 256, 512]},\n",
      "                'learning_rate': {'distribution': 'uniform',\n",
      "                                  'max': 0.1,\n",
      "                                  'min': 0.0001},\n",
      "                'optimizer': {'values': ['adam', 'sgd']}}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba1699b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m To avoid this, please fix the sweep config schema violations below:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 1. batch_size uses q_log_uniform, where min/max specify base-e exponents. Use q_log_uniform_values to specify limit values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: w5nnwjv3\n",
      "Sweep URL: https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/sweeps/w5nnwjv3\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"my-test-project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bd6f80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def train(config=None):\n",
    "    #Initialize a new wandb run\n",
    "    with wandb.init(config=config) as run:\n",
    "        config = run.config\n",
    "\n",
    "        loader = build_datasets(config.batch_size)\n",
    "        network = build_model(config.fc_layer_size, config.dropout)\n",
    "        optimizer = build_optimizer(network, config.optimizer, config.learning_rate)\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            avg_loss = train_epoch(network, loader, optimizer)\n",
    "            run.log({\"loss\": avg_loss, \"epoch\": epoch})\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83721850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_datasets(batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    sub_dataset = torch.utils.data.Subset(dataset, range(1000))\n",
    "    train_loader = torch.utils.data.DataLoader(sub_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader\n",
    "\n",
    "def build_network(fc_layer_size, dropout):\n",
    "    network = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(28*28, fc_layer_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(fc_layer_size, 10),\n",
    "        nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "    return network.to(device)\n",
    "\n",
    "\n",
    "def build_optimizer(network, optimizer, learning_rate):\n",
    "    if optimizer == 'sgd':\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer == 'adam':\n",
    "        optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def train_epoch(network, loader, optimizer):\n",
    "    cumu_loss = 0.0\n",
    "\n",
    "    for _, (data, target) in enumerate(loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = F.nll_loss(network(data), target)\n",
    "        cumu_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        wandb.log({\"loss\": loss.item()})\n",
    "\n",
    "    return cumu_loss / len(loader)    \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98d6ffc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 38w6w6qt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: -9223372036854776000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layer_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.012774811711284372\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\xt\\AppData\\Local\\Programs\\Microsoft VS Code\\wandb\\run-20251014_114502-38w6w6qt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/runs/38w6w6qt' target=\"_blank\">smart-sweep-1</a></strong> to <a href='https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/sweeps/w5nnwjv3' target=\"_blank\">https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/sweeps/w5nnwjv3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project' target=\"_blank\">https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/sweeps/w5nnwjv3' target=\"_blank\">https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/sweeps/w5nnwjv3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/runs/38w6w6qt' target=\"_blank\">https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/runs/38w6w6qt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\xt\\AppData\\Local\\Temp\\ipykernel_20180\\2282735746.py\", line 15, in train\n",
      "    loader = build_datasets(config.batch_size)\n",
      "  File \"C:\\Users\\xt\\AppData\\Local\\Temp\\ipykernel_20180\\4052138721.py\", line 9, in build_datasets\n",
      "    train_loader = torch.utils.data.DataLoader(sub_dataset, batch_size=batch_size, shuffle=True)\n",
      "  File \"c:\\Users\\xt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 394, in __init__\n",
      "    batch_sampler = BatchSampler(sampler, batch_size, drop_last)\n",
      "  File \"c:\\Users\\xt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 315, in __init__\n",
      "    raise ValueError(\n",
      "        f\"batch_size should be a positive integer value, but got batch_size={batch_size}\"\n",
      "    )\n",
      "ValueError: batch_size should be a positive integer value, but got batch_size=-9223372036854776000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smart-sweep-1</strong> at: <a href='https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/runs/38w6w6qt' target=\"_blank\">https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/runs/38w6w6qt</a><br> View project at: <a href='https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project' target=\"_blank\">https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251014_114502-38w6w6qt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\xt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "    ~~~~~~~~~~~~~~^^\n",
      "  File \"C:\\Users\\xt\\AppData\\Local\\Temp\\ipykernel_20180\\2282735746.py\", line 15, in train\n",
      "    loader = build_datasets(config.batch_size)\n",
      "  File \"C:\\Users\\xt\\AppData\\Local\\Temp\\ipykernel_20180\\4052138721.py\", line 9, in build_datasets\n",
      "    train_loader = torch.utils.data.DataLoader(sub_dataset, batch_size=batch_size, shuffle=True)\n",
      "  File \"c:\\Users\\xt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 394, in __init__\n",
      "    batch_sampler = BatchSampler(sampler, batch_size, drop_last)\n",
      "  File \"c:\\Users\\xt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 315, in __init__\n",
      "    raise ValueError(\n",
      "        f\"batch_size should be a positive integer value, but got batch_size={batch_size}\"\n",
      "    )\n",
      "ValueError: batch_size should be a positive integer value, but got batch_size=-9223372036854776000\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 38w6w6qt errored: batch_size should be a positive integer value, but got batch_size=-9223372036854776000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ma07p490 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: -9223372036854776000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layer_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.08200024223025958\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\xt\\AppData\\Local\\Programs\\Microsoft VS Code\\wandb\\run-20251014_114540-ma07p490</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/runs/ma07p490' target=\"_blank\">logical-sweep-2</a></strong> to <a href='https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/sweeps/w5nnwjv3' target=\"_blank\">https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/sweeps/w5nnwjv3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project' target=\"_blank\">https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/sweeps/w5nnwjv3' target=\"_blank\">https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/sweeps/w5nnwjv3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/runs/ma07p490' target=\"_blank\">https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/runs/ma07p490</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\xt\\AppData\\Local\\Temp\\ipykernel_20180\\2282735746.py\", line 15, in train\n",
      "    loader = build_datasets(config.batch_size)\n",
      "  File \"C:\\Users\\xt\\AppData\\Local\\Temp\\ipykernel_20180\\4052138721.py\", line 9, in build_datasets\n",
      "    train_loader = torch.utils.data.DataLoader(sub_dataset, batch_size=batch_size, shuffle=True)\n",
      "  File \"c:\\Users\\xt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 394, in __init__\n",
      "    batch_sampler = BatchSampler(sampler, batch_size, drop_last)\n",
      "  File \"c:\\Users\\xt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 315, in __init__\n",
      "    raise ValueError(\n",
      "        f\"batch_size should be a positive integer value, but got batch_size={batch_size}\"\n",
      "    )\n",
      "ValueError: batch_size should be a positive integer value, but got batch_size=-9223372036854776000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">logical-sweep-2</strong> at: <a href='https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/runs/ma07p490' target=\"_blank\">https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/runs/ma07p490</a><br> View project at: <a href='https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project' target=\"_blank\">https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251014_114540-ma07p490\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\xt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "    ~~~~~~~~~~~~~~^^\n",
      "  File \"C:\\Users\\xt\\AppData\\Local\\Temp\\ipykernel_20180\\2282735746.py\", line 15, in train\n",
      "    loader = build_datasets(config.batch_size)\n",
      "  File \"C:\\Users\\xt\\AppData\\Local\\Temp\\ipykernel_20180\\4052138721.py\", line 9, in build_datasets\n",
      "    train_loader = torch.utils.data.DataLoader(sub_dataset, batch_size=batch_size, shuffle=True)\n",
      "  File \"c:\\Users\\xt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 394, in __init__\n",
      "    batch_sampler = BatchSampler(sampler, batch_size, drop_last)\n",
      "  File \"c:\\Users\\xt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 315, in __init__\n",
      "    raise ValueError(\n",
      "        f\"batch_size should be a positive integer value, but got batch_size={batch_size}\"\n",
      "    )\n",
      "ValueError: batch_size should be a positive integer value, but got batch_size=-9223372036854776000\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ma07p490 errored: batch_size should be a positive integer value, but got batch_size=-9223372036854776000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cw2sh5l2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: -9223372036854776000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.046101258075115184\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\xt\\AppData\\Local\\Programs\\Microsoft VS Code\\wandb\\run-20251014_114550-cw2sh5l2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/runs/cw2sh5l2' target=\"_blank\">dulcet-sweep-3</a></strong> to <a href='https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/sweeps/w5nnwjv3' target=\"_blank\">https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/sweeps/w5nnwjv3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project' target=\"_blank\">https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/sweeps/w5nnwjv3' target=\"_blank\">https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/sweeps/w5nnwjv3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/runs/cw2sh5l2' target=\"_blank\">https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/runs/cw2sh5l2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\xt\\AppData\\Local\\Temp\\ipykernel_20180\\2282735746.py\", line 15, in train\n",
      "    loader = build_datasets(config.batch_size)\n",
      "  File \"C:\\Users\\xt\\AppData\\Local\\Temp\\ipykernel_20180\\4052138721.py\", line 9, in build_datasets\n",
      "    train_loader = torch.utils.data.DataLoader(sub_dataset, batch_size=batch_size, shuffle=True)\n",
      "  File \"c:\\Users\\xt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 394, in __init__\n",
      "    batch_sampler = BatchSampler(sampler, batch_size, drop_last)\n",
      "  File \"c:\\Users\\xt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 315, in __init__\n",
      "    raise ValueError(\n",
      "        f\"batch_size should be a positive integer value, but got batch_size={batch_size}\"\n",
      "    )\n",
      "ValueError: batch_size should be a positive integer value, but got batch_size=-9223372036854776000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dulcet-sweep-3</strong> at: <a href='https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/runs/cw2sh5l2' target=\"_blank\">https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project/runs/cw2sh5l2</a><br> View project at: <a href='https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project' target=\"_blank\">https://wandb.ai/emmad007-ned-university-of-engineering-and-technology/my-test-project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251014_114550-cw2sh5l2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\xt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "    ~~~~~~~~~~~~~~^^\n",
      "  File \"C:\\Users\\xt\\AppData\\Local\\Temp\\ipykernel_20180\\2282735746.py\", line 15, in train\n",
      "    loader = build_datasets(config.batch_size)\n",
      "  File \"C:\\Users\\xt\\AppData\\Local\\Temp\\ipykernel_20180\\4052138721.py\", line 9, in build_datasets\n",
      "    train_loader = torch.utils.data.DataLoader(sub_dataset, batch_size=batch_size, shuffle=True)\n",
      "  File \"c:\\Users\\xt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 394, in __init__\n",
      "    batch_sampler = BatchSampler(sampler, batch_size, drop_last)\n",
      "  File \"c:\\Users\\xt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 315, in __init__\n",
      "    raise ValueError(\n",
      "        f\"batch_size should be a positive integer value, but got batch_size={batch_size}\"\n",
      "    )\n",
      "ValueError: batch_size should be a positive integer value, but got batch_size=-9223372036854776000\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run cw2sh5l2 errored: batch_size should be a positive integer value, but got batch_size=-9223372036854776000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: To disable this check set WANDB_AGENT_DISABLE_FLAPPING=true\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, function=train, count=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
